<script>
    let fx_str = `
    INFO:torch._functorch._aot_autograd.dispatch_and_compile_graph.__aot_graphs:TRACED GRAPH
    ===== Forward graph 0 =====
    /data/users/chilli/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
	    def forward(self):
	        arg0_1: "f32[10, 20][20, 1]cuda:0"; arg1_1: "f32[20, 30][30, 1]cuda:0"; arg2_1: "f32[10, 30][30, 1]cuda:0";

	        arg0_1, arg1_1, arg2_1, = fx_pytree.tree_flatten_spec([], self._in_spec)
	         # File: /mnt/xarfuse/uid-659722/e8e2be2c-seed-nspid4026531836_cgpid3595376-ns-4026531841/caffe2/test/inductor/test_aot_inductor.py:607 in forward, code: x = a * 3.14
	        from_node=[('x', '<built-in function mul>')]
	        pass_source=['aot_export_module']
	        mul: "f32[10, 20][20, 1]cuda:0" = torch.ops.aten.mul.Tensor(arg0_1, 3.14);  arg0_1 = None

	        # No stacktrace found for following nodes
	        from_node=[('y', '<built-in method addmm of type object at 0x5623383579a0>'), ('addmm', 'aten.addmm.default')]
	        pass_source=['aot_export_module', 'pattern_matcher', 'pattern_matcher']
	        mm_default: "f32[10, 30][30, 1]cuda:0" = torch.ops.aten.mm.default(mul, arg1_1);  mul = arg1_1 = None
	        from_node=[('y', '<built-in method addmm of type object at 0x5623383579a0>'), ('addmm', 'aten.addmm.default')]
	        pass_source=['aot_export_module', 'pattern_matcher', 'pattern_matcher']
	        add_tensor: "f32[10, 30][30, 1]cuda:0" = torch.ops.aten.add.Tensor(mm_default, arg2_1);  mm_default = arg2_1 = None

	         # File: /mnt/xarfuse/uid-659722/e8e2be2c-seed-nspid4026531836_cgpid3595376-ns-4026531841/caffe2/test/inductor/test_aot_inductor.py:609 in forward, code: z = torch.nn.functional.gelu(y)
	        from_node=[('z', '<built-in function gelu>')]
	        pass_source=['aot_export_module']
	        mul_1: "f32[10, 30][30, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_tensor, 0.5)
	        from_node=[('z', '<built-in function gelu>')]
	        pass_source=['aot_export_module']
	        mul_2: "f32[10, 30][30, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_tensor, 0.7071067811865476);  add_tensor = None
	        from_node=[('z', '<built-in function gelu>')]
	        pass_source=['aot_export_module']
	        erf: "f32[10, 30][30, 1]cuda:0" = torch.ops.aten.erf.default(mul_2);  mul_2 = None
	        from_node=[('z', '<built-in function gelu>')]
	        pass_source=['aot_export_module']
	        add: "f32[10, 30][30, 1]cuda:0" = torch.ops.aten.add.Tensor(erf, 1);  erf = None
	        from_node=[('z', '<built-in function gelu>')]
	        pass_source=['aot_export_module']
	        mul_3: "f32[10, 30][30, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1, add);  mul_1 = add = None
	        return (mul_3,)
    `
    let code_str = `
    from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	import torch._inductor.kernel.mm_common

	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()


	# kernel path: /tmp/torchinductor_guorachel/tmp32lx2dsg/zv/czvozuqo7lvvoikiuoajx6n5i7av5a3qcxzrztnuedw2lw2yd2c3.py
	# Topologically Sorted Source Nodes: [x], Original ATen: [aten.mul]
	# Source node to ATen node mapping:
	#   x => mul
	# Graph fragment:
	#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 3.14), kwargs = {})
	triton_poi_fused_mul_0 = async_compile.triton('triton_poi_fused_mul_0', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor

	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
	triton_helpers.set_driver_to_gpu()

	@triton_heuristics.pointwise(
	    size_hints=[256],
	    filename=__file__,
	    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'B34EEE23CB25737B0783931EA8D99A342C090EE4AEACFCE242233C64593BF56F', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_poi_fused_mul_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 200
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = xindex < xnumel
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), xmask)
	    tmp1 = 3.14
	    tmp2 = tmp0 * tmp1
	    tl.store(out_ptr0 + (x0), tmp2, xmask)
	''', device_str='cuda')


	# kernel path: /tmp/torchinductor_guorachel/tmp32lx2dsg/qr/cqr6otxgidgi247hptpvihmqcmr6nvmcufq6akh556ypshbeg3hd.py
	# Topologically Sorted Source Nodes: [x, y], Original ATen: [aten.mul, aten.addmm]
	# Source node to ATen node mapping:
	#   x => mul
	#   y => mm_default
	# Graph fragment:
	#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 3.14), kwargs = {})
	#   %mm_default : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%mul, %arg1_1), kwargs = {})
	triton_tem_fused_addmm_mul_1 = async_compile.triton('triton_tem_fused_addmm_mul_1', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor

	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

	@triton_heuristics.template(
	    num_stages=5,
	    num_warps=2,
	    triton_meta={'signature': {'arg_A': '*fp32', 'arg_B': '*fp32', 'out_ptr0': '*fp32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_mul_1', 'backend_hash': 'B34EEE23CB25737B0783931EA8D99A342C090EE4AEACFCE242233C64593BF56F', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
	)
	@triton.jit
	def triton_tem_fused_addmm_mul_1(arg_A, arg_B, out_ptr0):
	    GROUP_M : tl.constexpr = 8
	    EVEN_K : tl.constexpr = False
	    ALLOW_TF32 : tl.constexpr = False
	    ACC_TYPE : tl.constexpr = tl.float32
	    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
	    BLOCK_M : tl.constexpr = 16
	    BLOCK_N : tl.constexpr = 32
	    BLOCK_K : tl.constexpr = 32
	    A = arg_A
	    B = arg_B

	    M = 10
	    N = 30
	    K = 20
	    if M * N == 0:
	        # early exit due to zero-size input(s)
	        return
	    stride_am = 20
	    stride_ak = 1
	    stride_bk = 30
	    stride_bn = 1

	    # based on triton.ops.matmul
	    pid = tl.program_id(0)
	    grid_m = (M + BLOCK_M - 1) // BLOCK_M
	    grid_n = (N + BLOCK_N - 1) // BLOCK_N

	    # re-order program ID for better L2 performance
	    width = GROUP_M * grid_n
	    group_id = pid // width
	    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
	    pid_m = group_id * GROUP_M + (pid % group_size)
	    pid_n = (pid % width) // (group_size)

	    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
	    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
	    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
	        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
	    else:
	        ram = rm % M
	    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
	        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
	    else:
	        rbn = rn % N
	    rk = tl.arange(0, BLOCK_K)
	    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
	    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

	    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
	    for k in range(K, 0, -BLOCK_K):
	        if EVEN_K:
	            a = tl.load(A)
	            b = tl.load(B)
	        else:
	            a = tl.load(A, mask=rk[None, :] < k, other=0.)
	            b = tl.load(B, mask=rk[:, None] < k, other=0.)
	        if B_PROLOGUE_CAST_TYPE is not None:
	            b = b.to(B_PROLOGUE_CAST_TYPE)
	        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
	        A += BLOCK_K * stride_ak
	        B += BLOCK_K * stride_bk

	    # rematerialize rm and rn to save registers
	    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
	    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
	    idx_m = rm[:, None]
	    idx_n = rn[None, :]
	    mask = (idx_m < M) & (idx_n < N)

	    # inductor generates a suffix
	    xindex = idx_n + (30*idx_m)
	    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
	''', device_str='cuda')
	meta0 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 16, 'BLOCK_N': 32, 'BLOCK_K': 32}


	# kernel path: /tmp/torchinductor_guorachel/tmp32lx2dsg/pq/cpqzpmso4l3ajn6aprvh52xi47tauanfonsnqqkmftbelt2y62gq.py
	# Topologically Sorted Source Nodes: [y, z], Original ATen: [aten.addmm, aten.gelu]
	# Source node to ATen node mapping:
	#   y => add_tensor
	#   z => add, erf, mul_1, mul_2, mul_3
	# Graph fragment:
	#   %add_tensor : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %arg2_1), kwargs = {})
	#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, 0.5), kwargs = {})
	#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, 0.7071067811865476), kwargs = {})
	#   %erf : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_2,), kwargs = {})
	#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf, 1), kwargs = {})
	#   %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1, %add), kwargs = {})
	triton_poi_fused_addmm_gelu_2 = async_compile.triton('triton_poi_fused_addmm_gelu_2', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor

	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
	triton_helpers.set_driver_to_gpu()

	@triton_heuristics.pointwise(
	    size_hints=[512],
	    filename=__file__,
	    triton_meta={'signature': {'in_out_ptr0': '*fp32', 'in_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_gelu_2', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'B34EEE23CB25737B0783931EA8D99A342C090EE4AEACFCE242233C64593BF56F', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_poi_fused_addmm_gelu_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 300
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = xindex < xnumel
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)
	    tmp1 = tl.load(in_ptr0 + (x0), xmask)
	    tmp2 = tmp0 + tmp1
	    tmp3 = 0.5
	    tmp4 = tmp2 * tmp3
	    tmp5 = 0.7071067811865476
	    tmp6 = tmp2 * tmp5
	    tmp7 = libdevice.erf(tmp6)
	    tmp8 = 1.0
	    tmp9 = tmp7 + tmp8
	    tmp10 = tmp4 * tmp9
	    tl.store(in_out_ptr0 + (x0), tmp10, xmask)
	''', device_str='cuda')


	async_compile.wait(globals())
	del async_compile

	def call(args):
	    arg0_1, arg1_1, arg2_1 = args
	    args.clear()
	    assert_size_stride(arg0_1, (10, 20), (20, 1))
	    assert_size_stride(arg1_1, (20, 30), (30, 1))
	    assert_size_stride(arg2_1, (10, 30), (30, 1))

	    for kernel in globals().values():
	        if isinstance(kernel, torch._inductor.runtime.triton_heuristics.CachingAutotuner):
	            kernel.cuda_kernel_saved = False
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((10, 20), (20, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.mul]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_mul_0.run(arg0_1, buf0, 200, grid=grid(200), stream=stream0)
	        del arg0_1
	        buf1 = empty_strided_cuda((10, 30), (30, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [x, y], Original ATen: [aten.mul, aten.addmm]
	        triton_tem_fused_addmm_mul_1.run(buf0, arg1_1, buf1, grid=torch._inductor.kernel.mm_common.mm_grid(10, 30, meta0), stream=stream0)
	        del arg1_1
	        del buf0
	        buf2 = buf1; del buf1  # reuse
	        # Topologically Sorted Source Nodes: [y, z], Original ATen: [aten.addmm, aten.gelu]
	        triton_poi_fused_addmm_gelu_2.run(buf2, arg2_1, 300, grid=grid(300), stream=stream0)
	        del arg2_1

	    for kernel in globals().values():
	        if isinstance(kernel, torch._inductor.runtime.triton_heuristics.CachingAutotuner):
	            if not kernel.cuda_kernel_saved:
	                if len(kernel.launchers) == 0:
	                    kernel.precompile()
	                kernel.save_gpu_kernel(
	                    grid=(0, 0, 0),   # use dummy grid
	                    stream="stream",  # use dummy stream
	                    launcher=kernel.launchers[0],
	                )
	    return (buf2, )


	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = rand_strided((10, 20), (20, 1), device='cuda:0', dtype=torch.float32)
	    arg1_1 = rand_strided((20, 30), (30, 1), device='cuda:0', dtype=torch.float32)
	    arg2_1 = rand_strided((10, 30), (30, 1), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([arg0_1, arg1_1, arg2_1])
	    return print_performance(fn, times=times, repeat=repeat)


	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
    `;

function getGraph() {
        return fx_str.split('\n');
        }

function getCode() {
    return code_str.split('\n');
}

</script>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complete Bidirectional Highlighting with Draggable Divider</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            height: 100vh;
            overflow: hidden;
        }
        .url-inputs {
            display: flex;
            padding: 10px;
            background-color: #f0f0f0;
        }
        .url-input {
            flex: 1;
            margin-right: 10px;
        }
        .url-input input {
            width: 100%;
            padding: 5px;
        }
        .editor-container {
            display: flex;
            flex: 1;
            overflow: hidden;
        }
        .editor {
            height: 100%;
            overflow-y: auto;
            border: 1px solid #ddd;
            padding: 10px;
            box-sizing: border-box;
            flex: 1;
            font-family: monospace;
        }
        .line {
            padding: 2px 5px;
            cursor: pointer;
            white-space: nowrap;
        }
        .highlight {
            background-color: yellow;
        }
        .mapped-line {
            font-weight: bold;
        }
        .line-number {
            color: #888;
            display: inline-block;
            width: 30px;
            text-align: right;
            margin-right: 10px;
        }
        .divider {
            width: 10px;
            background-color: #ccc;
            cursor: col-resize;
        }
        .line-content {
            white-space: pre;
            display: inline;
        }
    </style>
</head>
<body>
    <div class="url-inputs">
        <div class="url-input">
            <input type="text" id="fxGraphUrl" placeholder="Paste FX Graph URL here">
            <button onclick="loadFile('fxGraph')">Load FX Graph</button>
        </div>
        <div class="url-input">
            <input type="text" id="generatedCodeUrl" placeholder="Paste Generated Code URL here">
            <button onclick="loadFile('generatedCode')">Load Generated Code</button>
        </div>
        <div class="url-input">
            <button onclick="updateUrlWithInputs()">Update URL with Inputs</button>
        </div>
        <script>
            function updateUrlWithInputs() {
                const fxGraphUrl = document.getElementById('fxGraphUrl').value;
                const generatedCodeUrl = document.getElementById('generatedCodeUrl').value;

                let newUrl = window.location.origin + window.location.pathname + '?';

                if (fxGraphUrl) {
                    newUrl += 'fxGraphUrl=' + encodeURIComponent(fxGraphUrl);
                }

                if (generatedCodeUrl) {
                    if (fxGraphUrl) newUrl += '&';
                    newUrl += 'generatedCodeUrl=' + encodeURIComponent(generatedCodeUrl);
                }

                window.history.pushState({}, '', newUrl);
            }
        </script>


        <script>
            // Load from URL if present
            window.onload = function() {
                const urlParams = new URLSearchParams(window.location.search);
                const fxGraphUrl = urlParams.get('fxGraphUrl');
                const generatedCodeUrl = urlParams.get('generatedCodeUrl');

                if (fxGraphUrl) {
                    document.getElementById('fxGraphUrl').value = fxGraphUrl;
                    loadFile('fxGraph');
                }
                if (generatedCodeUrl) {
                    document.getElementById('generatedCodeUrl').value = generatedCodeUrl;
                    loadFile('generatedCode');
                }
            };
        </script>
    </div>
    <div class="editor-container">
        <div id="fxGraph" class="editor"></div>
        <div id="divider" class="divider"></div>
        <div id="generatedCode" class="editor"></div>
    </div>

    <!-- <script src="data.js"></script> -->
    <script>

        let jsonString = '{"triton_poi_fused_mul_0": ["mul"], "triton_tem_fused_addmm_mul_1": ["mm_default", "mul"], "triton_poi_fused_addmm_gelu_2": ["add_tensor", "add", "erf", "mul_1", "mul_2", "mul_3"]}'
        const jsonData = JSON.parse(jsonString)

        function computeSourceMaps(jsonData, fxGraph, generatedCode) {
            const leftToRight = [];
            const rightToLeft = [];
            const fxLines = fxGraph;
            const codeLines = generatedCode;

            // Initialize leftToRight and rightToLeft as arrays
            for (let i = 0; i < fxLines.length; i++) {
                leftToRight[i] = [];
            }
            for (let i = 0; i < codeLines.length; i++) {
                rightToLeft[i] = [];
            }

            // parse the jsonData here
            const transformedObject = {};
            // Iterate over the original object and reverse the key-value relationship
            for (let [key, values] of Object.entries(jsonData)) {
                values.forEach(value => {
                    if (transformedObject[value]) {
                        transformedObject[value].push(key)
                    } else {
                        transformedObject[value] = [key]
                    }
                });
            }

            // Regular expression to match node names in fx graph lines
            const nodeNameRegex = /^\s*(\w+):/;

            // Iterate through each line in the fx graph
            fxLines.forEach((fxLine, fxIndex) => {
                const match = fxLine.match(nodeNameRegex);
                if (match) {
                    const nodeName = match[1];
                    // question: what do we do about the ops that map to multiple triton kernels
                    const kernelNames = nodeName in transformedObject ? transformedObject[nodeName] : null;
                    // Search for the node name in the generated code
                    // TODO: map it to the entire triton kernel codegen wrapper code
                    codeLines.forEach((codeLine, codeIndex) => {
                        if (kernelNames && kernelNames.some(kernelName => codeLine.includes(kernelName)) && codeLine.includes("= async_compile.triton(")) {
                                leftToRight[fxIndex].push(codeIndex);
                                rightToLeft[codeIndex].push(fxIndex);
                            }
                        });
                }
            });


            return { leftToRight, rightToLeft };
        }


        let fxGraphData = getGraph();
        let codeData = getCode();
        let { leftToRight, rightToLeft } = computeSourceMaps(jsonData, fxGraphData, codeData);
        async function loadFile(editorId) {
            const urlInput = document.getElementById(`${editorId}Url`);
            const url = urlInput.value.trim();
            if (!url) {
                alert('Please enter a valid URL');
                return;
            }


            try {
                const content = await fetch(url)
                .then(response => response.text())
                .then(html => {
                    // Create a new DOMParser
                    const parser = new DOMParser();
                    // Parse the HTML string
                    const doc = parser.parseFromString(html, 'text/html');
                    // Get the text content
                    const textContent = doc.body.textContent;
                    return textContent;
                })
                .catch(error => console.error('Error:', error));
                const lines = content.split('\n').filter(line => line !== '');

                if (editorId === 'fxGraph') {
                    fxGraphData = lines;
                } else {
                    codeData = lines;
                }
                console.log(fxGraphData, codeData);
                const mappings = computeSourceMaps(fxGraphData, codeData);
                console.log(mappings);
                leftToRight = mappings.leftToRight;
                rightToLeft = mappings.rightToLeft;
                populateEditor(fxGraph, fxGraphData, leftToRight);
                populateEditor(generatedCode, codeData, rightToLeft);

            } catch (error) {
                console.error('Error loading file:', error);
                alert('Error loading file. Please check the URL and try again.');
            }
        }

        function highlightLines(sourceEditor, targetEditor, mapping, lineNumber) {
            targetEditor.querySelectorAll('.highlight').forEach(el => el.classList.remove('highlight'));
            const correspondingLines = mapping[lineNumber - 1] || [];
            correspondingLines.forEach(line => {
                const targetLine = targetEditor.children[line];
                if (targetLine) {
                    targetLine.classList.add('highlight');
                }
            });
        }

        function handleMouseMove(event, sourceEditor, targetEditor, mapping) {
            const target = event.target.closest('.line');
            if (target) {
                const lineNumber = Array.from(sourceEditor.children).indexOf(target) + 1;
                highlightLines(sourceEditor, targetEditor, mapping, lineNumber);
            }
        }

        function handleClick(event, sourceEditor, targetEditor, mapping) {
            const target = event.target.closest('.line');
            if (target) {
                const lineNumber = Array.from(sourceEditor.children).indexOf(target) + 1;
                const correspondingLines = mapping[lineNumber - 1] || [];
                if (correspondingLines.length > 0) {
                    const firstCorrespondingLine = correspondingLines[0];
                    const targetLine = targetEditor.children[firstCorrespondingLine - 1];
                    if (targetLine) {
                        targetLine.scrollIntoView({ behavior: 'smooth', block: 'center' });
                        highlightLines(sourceEditor, targetEditor, mapping, lineNumber);
                    }
                }
            }
        }


        const fxGraph = document.getElementById('fxGraph');
        const generatedCode = document.getElementById('generatedCode');
        const divider = document.getElementById('divider');

        // Function to set initial editor widths
        function setInitialEditorWidths() {
            const containerWidth = document.querySelector('.editor-container').offsetWidth;
            const editorWidth = (containerWidth - divider.offsetWidth) / 2;
            fxGraph.style.width = `${editorWidth}px`;
            generatedCode.style.width = `${editorWidth}px`;
        }

        // Call this function when the page loads
        window.addEventListener('load', setInitialEditorWidths);

        // Also call it when the window is resized
        window.addEventListener('resize', setInitialEditorWidths);

        fxGraphData.forEach((line, index) => {
            const div = document.createElement('div');
            div.className = 'line';
            div.innerHTML = `<span class="line-number">${index + 1}</span>${line}`;
            fxGraph.appendChild(div);
        });

        codeData.forEach((line, index) => {
            const div = document.createElement('div');
            div.className = 'line';
            div.innerHTML = `<span class="line-number">${index + 1}</span>${line}`;
            generatedCode.appendChild(div);
        });

        function populateEditor(editor, data, mapping) {
            editor.innerHTML = '';
            data.forEach((line, index) => {
                const div = document.createElement('div');
                div.className = 'line';
                if (mapping[index] && mapping[index].length > 0) {
                    div.classList.add('mapped-line');
                }
                const lineNumber = document.createElement('span');
                lineNumber.className = 'line-number';
                lineNumber.textContent = index + 1;
                div.appendChild(lineNumber);
                const lineContent = document.createElement('span');
                lineContent.className = 'line-content';
                lineContent.textContent = line;
                div.appendChild(lineContent);

                editor.appendChild(div);
            });
        }

        // Populate editors with shading
        populateEditor(fxGraph, fxGraphData, leftToRight);
        populateEditor(generatedCode, codeData, rightToLeft);

        // Modify the draggable functionality
        let isDragging = false;
        let startX, startLeftWidth;

        divider.addEventListener('mousedown', (e) => {
            isDragging = true;
            startX = e.clientX;
            startLeftWidth = fxGraph.offsetWidth;
        });

        document.addEventListener('mousemove', (e) => {
            if (!isDragging) return;
            const dx = e.clientX - startX;
            const newLeftWidth = startLeftWidth + dx;
            const containerWidth = document.querySelector('.editor-container').offsetWidth;

            if (newLeftWidth > 100 && newLeftWidth < containerWidth - 100) {
                fxGraph.style.width = `${newLeftWidth}px`;
                generatedCode.style.width = `${containerWidth - newLeftWidth - divider.offsetWidth}px`;
            }
        });

        document.addEventListener('mouseup', () => {
            isDragging = false;
        });


        // Event listeners for highlighting and scrolling
        fxGraph.addEventListener('mousemove', (event) => handleMouseMove(event, fxGraph, generatedCode, leftToRight));
        generatedCode.addEventListener('mousemove', (event) => handleMouseMove(event, generatedCode, fxGraph, rightToLeft));

        fxGraph.addEventListener('click', (event) => handleClick(event, fxGraph, generatedCode, leftToRight));
        generatedCode.addEventListener('click', (event) => handleClick(event, generatedCode, fxGraph, rightToLeft));

        fxGraph.addEventListener('mouseout', () => generatedCode.querySelectorAll('.highlight').forEach(el => el.classList.remove('highlight')));
        generatedCode.addEventListener('mouseout', () => fxGraph.querySelectorAll('.highlight').forEach(el => el.classList.remove('highlight')));
    </script>
</body>
</html>
